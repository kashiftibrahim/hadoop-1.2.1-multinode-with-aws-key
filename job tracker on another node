The Below script is made by considering that linux user have launch 4 Ec2 cluster on ubuntu Os with the AWS key name a.pem
go to the terminal by pressing crt+T

$ chmod 400 /Download/a.pem

$ scp -i a.pem a.pem ubuntu@52.18.223.48(ip address of your master node):/home/ubuntu/.ssh

Once you are connected to your EC2 just fire the following commond.

$ nano ~/.profile
eval `ssh-agent` ssh-add /home/ubuntu/.ssh/a.pem
ctrl+x y to save and exit from nano

$ source ~/.profile

$ sudo nano /etc/hosts
(please write the FQDN address of your EC2 Clusters)
172.31.23.8  ip-172-31-23-8.eu-west-1.compute.internal  nn
172.31.23.7  ip-172-31-23-7.eu-west-1.compute.internal  snn
172.31.23.10  ip-172-31-23-10.eu-west-1.compute.internal  dn1
172.31.23.9  ip-172-31-23-9.eu-west-1.compute.internal  dn2
ctrl+x y to save and exit from nano

$ ssh  snn 
$ sudo nano /etc/hosts
172.31.23.8  ip-172-31-23-8.eu-west-1.compute.internal  nn
172.31.23.7  ip-172-31-23-7.eu-west-1.compute.internal  snn
172.31.23.10  ip-172-31-23-10.eu-west-1.compute.internal  dn1
172.31.23.9  ip-172-31-23-9.eu-west-1.compute.internal  dn2
ctrl+x y to save and exit from nano
$ exit

$ ssh  dn1 
$ sudo nano /etc/hosts
172.31.23.8  ip-172-31-23-8.eu-west-1.compute.internal  nn
172.31.23.7  ip-172-31-23-7.eu-west-1.compute.internal  snn
172.31.23.10  ip-172-31-23-10.eu-west-1.compute.internal  dn1
172.31.23.9  ip-172-31-23-9.eu-west-1.compute.internal  dn2
ctrl+x y to save and exit from nano
$ exit

$ ssh  dn2 
$ sudo nano /etc/hosts
172.31.23.8  ip-172-31-23-8.eu-west-1.compute.internal  nn
172.31.23.7  ip-172-31-23-7.eu-west-1.compute.internal  snn
172.31.23.10  ip-172-31-23-10.eu-west-1.compute.internal  dn1
172.31.23.9  ip-172-31-23-9.eu-west-1.compute.internal  dn2
ctrl+x y to save and exit from nano
$ exit

$ sudo apt-get install dsh
$ cd /etc/dsh
$ sudo nano machines.list
#localhost
nn
snn 
dn1
dn2
ctrl+x y to save and exit from nano

$ nano ~/.bashrc
export HADOOP_PREFIX=/usr/local/hadoop/
export PATH=$PATH:$HADOOP_PREFIX/bin
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export PATH=$PATH:$JAVA_HOME
ctrl+x y to save and exit from nano

$ scp .bashrc .profile  ubuntu@snn:/home/ubuntu
$ scp .bashrc .profile  ubuntu@dn1:/home/ubuntu
$ scp .bashrc .profile  ubuntu@dn2:/home/ubuntu
$ scp .ssh/a.pem ubuntu@snn:/home/ubuntu/.ssh
$ scp .ssh/a.pem ubuntu@dn1:/home/ubuntu/.ssh
$ scp .ssh/a.pem ubuntu@dn2:/home/ubuntu/.ssh

$ dsh -a bash
$ dsh -a source ~/.profile

$ dsh -a wget http://apache.mirror.gtcomm.net/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz
$ dsh -a tar -xzvf hadoop-1.2.1.tar.gz
$ dsh -a sudo mv hadoop-1.2.1 /usr/local/hadoop
$ dsh -a sudo apt-get update
$ dsh -a sudo apt-get install openjdk-7-jdk  -y

$ nano /usr/local/hadoop/conf/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_OPTS=-Djava.net.preferIPV4Stack=true
ctrl+x y to save and exit from nano

$ nano /usr/local/hadoop/conf/core-site.xml
<property>
<name>fs.default.name</name>
<value>hdfs://nn:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/usr/local/hadoop/tmp</value>
</property>
ctrl+x y to save and exit from nano

$ nano /usr/local/hadoop/conf/hdfs-site.xml
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
<property> 
<name>dfs.permissions</name> 
<value>true</value> 
</property>
ctrl+x y to save and exit from nano

$ nano /usr/local/hadoop/conf/mapred-site.xml
<property>
<name>mapred.job.tracker</name>
<value>hdfs://jt:9001</value>
</property>
ctrl+x y to save and exit from nano

$ cd /usr/local/hadoop/conf/
$ scp  hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml ubuntu@snn:/usr/local/hadoop/conf/
$ scp  hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml ubuntu@dn1:/usr/local/hadoop/conf/
$ scp  hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml ubuntu@dn2:/usr/local/hadoop/conf/

$ nano /usr/local/hadoop/conf/masters
snn
ctrl+x y to save and exit from nano

$ nano /usr/local/hadoop/conf/slaves
dn1 
dn2
ctrl+x y to save and exit from nano

$ dsh -a mkdir /usr/local/hadoop/tmp
$ dsh -a exec bash

$ hadoop namenode -format

---------------starting hadoop services-------------------
start-all.sh 

jps

